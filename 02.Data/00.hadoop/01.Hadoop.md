---
title: Hadoop
date:
tags:
- Hadoop
---
<details>
<summary>点击展开目录</summary>
<!-- TOC -->

- [关于](#关于)
- [HDFS](#hdfs)
    - [hdfs操作](#hdfs操作)
    - [Hadoop shell](#hadoop-shell)
- [YARN](#yarn)
    - [YARN环境搭建](#yarn环境搭建)
- [MapReduce](#mapreduce)
    - [direct方式运行](#direct方式运行)
    - [spark方式运行](#spark方式运行)
    - [flink方式运行](#flink方式运行)
- [启动方式总结](#启动方式总结)
- [Q&A](#qa)

<!-- /TOC -->
</details>

## 关于

目录结构:

```
├── bin
├── etc
├── include
├── lib
├── libexec
├── sbin
└── share
```

* 分布式文件系统: HDFS
* 分布式资源调度: YARN
* 分布式计算: MapReduce

> 推荐使用cdh版本, [为什么](http://blog.csdn.net/cnhk1225/article/details/50357744), [官网](http://archive.cloudera.com/cdh5/cdh/5/)

## HDFS

1 Master(NameNode/NN) 带 N个 workers(DataNode/DN)

1个文件会被拆分成多个Block, blocksize: 128M

NameNode:

1. 管理文件系统命名空间
2. 负责元数据(文件的名称, 副本系数, Block存放的DN)的管理
3. 规范客户端对文件的访问, 负责客户端请求的响应
4. 执行文件系统操作, 如重命名, 关闭和打开的文件和目录

DataNode:

1. 存储用户的文件对应的数据块(Block)
2. 要定期向NN发送心跳信息, 汇报本身及其所有的block信息, 健康状况
3. 根据名称节点的指令执行操作

> A typical deployment has a dedicated machine that runs only the NameNode software.
> Each of the other machines in the cluster runs one instance of the DataNode software.
> The architecture does not preclude running multiple DataNodes on the same machine
> but in a real deployment that is rarely the case.

NameNode + N个DataNode

建议: NN和DN部署在不同的节点上

`replication factor`: 副本系数, 副本因子

> All blocks in a file except the last block are the same size

### hdfs操作

```bash
hdfs dfs -ls /
hdfs dfs -mkdir -p /user/hadoop/
hdfs dfs -put /home/file.txt /user/input/
hdfs dfs -get /user/output/ /home/hadoop_tp/
```

> `$HADOOP_HOME/bin/hdfs dfs`等价于`$HADOOP_HOME/bin/hadoop fs`

```bash
./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.4.jar wordcount /home/input /home/output
```

### Hadoop shell

Java API操作HDFS文件

文件    1    40762    hdfs://localhost:8020/install.log

问题:

我们已经在hdfs-site.xml中设置了副本系数为1, 为什么此时查询文件看到的3呢?

如果你是通过hdfs shell的方式put的上去的那么, 才采用默认的副本系数1

如果我们是java api上传上去的, 在本地我们并没有手工设置副本系数, 所以否则采用的是hadoop自己的副本系数

## YARN

YARN: 不同计算框架可以共享同一个HDFS集群上的数据, 享受整体的资源调度

YARN架构:
**ResourceManager: RM**

整个集群同一时间提供服务的RM只有一个, 负责集群资源的统一管理和调度
处理客户端的请求: 提交一个作业, 杀死一个作业
监控我们的NM, 一旦某个NM挂了, 那么该NM上运行的任务需要告诉我们的AM来如何进行处理

**NodeManager: NM**

整个集群中有多个, 负责自己本身节点资源管理和使用
定时向RM汇报本节点的资源使用情况
接收并处理来自RM的各种命令:启动Container
处理来自AM的命令
单个节点的资源管理

**ApplicationMaster: AM**

每个应用程序对应一个:MR, Spark, 负责应用程序的管理
为应用程序向RM申请资源(core, memory), 分配给内部task
需要与NM通信:启动/停止task, task是运行在container里面, AM也是运行在container里面

**Container**

封装了CPU, Memory等资源的一个容器
是一个任务运行环境的抽象

**Client**

提交作业
查询作业的运行进度
杀死作业

### YARN环境搭建

**使用**

提交mr作业到YARN上运行:
/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar

hadoop jar

hadoop jar hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar pi 2 3

wordcount: 统计文件中每个单词出现的次数

## MapReduce

借助于分布式计算框架来解决了: MapReduce

分而治之


(input) <k1, v1> -> map -> <k2, v2> -> combine -> <k2, v2> -> reduce -> <k3, v3> (output)

核心概念
Split:交由MapReduce作业来处理的数据块, 是MapReduce中最小的计算单元
    HDFS:blocksize 是HDFS中最小的存储单元  128M
    默认情况下:他们两是一一对应的, 当然我们也可以手工设置他们之间的关系(不建议)


InputFormat:
    将我们的输入数据进行分片(split):  InputSplit[] getSplits(JobConf job, int numSplits) throws IOException;
    TextInputFormat: 处理文本格式的数据

OutputFormat: 输出


MapReduce1.x的架构

1)JobTracker: JT
    作业的管理者      管理的
    将作业分解成一堆的任务:Task(MapTask和ReduceTask)
    将任务分派给TaskTracker运行
    作业的监控, 容错处理(task作业挂了, 重启task的机制)
    在一定的时间间隔内, JT没有收到TT的心跳信息, TT可能是挂了, TT上运行的任务会被指派到其他TT上去执行

2)TaskTracker: TT
    任务的执行者      干活的
    在TT上执行我们的Task(MapTask和ReduceTask)
    会与JT进行交互:执行/启动/停止作业, 发送心跳信息给JT

3)MapTask
    自己开发的map任务交由该Task出来
    解析每条记录的数据, 交给自己的map方法处理
    将map的输出结果写到本地磁盘(有些作业只仅有map没有reduce==>HDFS)

4)ReduceTask
    将Map Task输出的数据进行读取
    按照数据进行分组传给我们自己编写的reduce方法处理
    输出结果写到HDFS

JobTracker: 负责资源管理和作业调度
TaskTracker:
    定期向JT汇报本节点的健康状况, 资源使用情况, 作业执行情况;
    接收来自JT的命令:启动任务/杀死任务

使用IDEA+Maven开发wc:
1)开发
2)编译:mvn clean package -DskipTests
3)上传到服务器:scp target/hadoop-train-1.0.jar hadoop@localhost:~/lib
4)运行

hadoop jar /home/hadoop/lib/hadoop-train-1.0.jar com.imooc.hadoop.mapreduce.WordCountApp hdfs://localhost:8020/hello.txt hdfs://localhost:8020/output/wc

相同的代码和脚本再次执行, 会报错
security.UserGroupInformation:
PriviledgedActionException as:hadoop (auth:SIMPLE) cause:
org.apache.hadoop.mapred.FileAlreadyExistsException:
Output directory hdfs://localhost:8020/output/wc already exists
Exception in thread "main" org.apache.hadoop.mapred.FileAlreadyExistsException:
Output directory hdfs://localhost:8020/output/wc already exists

在MR中, 输出文件是不能事先存在的
1)先手工通过shell的方式将输出文件夹先删除
    hadoop fs -rm -r /output/wc
2) 在代码中完成自动删除功能: 推荐大家使用这种方式
    Path outputPath = new Path(args[1]);
    FileSystem fileSystem = FileSystem.get(configuration);
    if(fileSystem.exists(outputPath)){
        fileSystem.delete(outputPath, true);
        System.out.println("output file exists, but is has deleted");
    }

Combiner
hadoop jar /home/hadoop/lib/hadoop-train-1.0.jar com.imooc.hadoop.mapreduce.CombinerApp hdfs://localhost:8020/hello.txt hdfs://localhost:8020/output/wc

使用场景:
    求和, 次数   +
    平均数  X

Partitioner
hadoop jar /home/hadoop/lib/hadoop-train-1.0.jar com.imooc.hadoop.mapreduce.ParititonerApp hdfs://localhost:8020/partitioner hdfs://localhost:8020/output/partitioner

用户行为日志:用户每次访问网站时所有的行为数据(访问, 浏览, 搜索, 点击...)
    用户行为轨迹, 流量日志


日志数据内容:
1)访问的系统属性: 操作系统, 浏览器等等
2)访问特征:点击的url, 从哪个url跳转过来的(referer), 页面上的停留时间等
3)访问信息:session_id, 访问ip(访问城市)等

2013-05-19 13:00:00     http://www.taobao.com/17/?tracker_u=1624169&type=1      B58W48U4WKZCJ5D1T3Z9ZY88RU7QA7B1        http://hao.360.cn/      1.196.34.243

数据处理流程

1)数据采集
    Flume: web日志写入到HDFS

2)数据清洗
    脏数据
    Spark, Hive, MapReduce 或者是其他的一些分布式计算框架
    清洗完之后的数据可以存放在HDFS(Hive/Spark SQL)

3)数据处理
    按照我们的需要进行相应业务的统计和分析
    Spark, Hive, MapReduce 或者是其他的一些分布式计算框架

4)处理结果入库
    结果可以存放到RDBMS, NoSQL

5)数据的可视化
    通过图形化展示的方式展现出来:饼图, 柱状图, 地图, 折线图
    ECharts, HUE, Zeppelin

UserAgent

hadoop jar /home/hadoop/lib/hadoop-train-1.0-jar-with-dependencies.jar com.imooc.hadoop.project.LogApp /10000_access.log /browserout

spark启动:spark-shell --master local[2]

spark实现wc:
val file = sc.textFile("file:///home/hadoop/data/hello.txt")
val a = file.flatMap(line => line.split(" "))
val b = a.map(word => (word, 1))
Array((hadoop, 1), (welcome, 1), (hadoop, 1), (hdfs, 1), (mapreduce, 1), (hadoop, 1), (hdfs, 1))

val c = b.reduceByKey(_ + _)
    Array((mapreduce, 1), (welcome, 1), (hadoop, 3), (hdfs, 2))

sc.textFile("file:///home/hadoop/data/hello.txt").flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_ + _).collect

Flink运行
./bin/flink run ./examples/batch/WordCount.jar \
--input file:///home/hadoop/data/hello.txt --output file:///home/hadoop/tmp/flink_wc_output

Beam运行:

### direct方式运行

```shell
mvn compile exec:java -Dexec.mainClass=org.apache.beam.examples.WordCount \
-Dexec.args="--inputFile=/home/hadoop/data/hello.txt --output=counts" \
-Pdirect-runner
```

### spark方式运行

```shell
mvn compile exec:java -Dexec.mainClass=org.apache.beam.examples.WordCount \
-Dexec.args="--runner=SparkRunner --inputFile=/home/hadoop/data/hello.txt --output=counts" -Pspark-runner
```

### flink方式运行


## 启动方式总结

```shell
# 方式1:
# hdfs
hadoop-daemon.sh start|stop namenode|datanode|secondarynamenode
# yarn
yarn-daemon.sh start|stop resourcemanager|nodemanager
# mapreduce
mr-historyserver-daemon.sh start|stop historyserver
方式2:
# hdfs
start-dfs.sh | stop-dfs.sh
# yarn
start-yarn.sh | stop-yarn.sh
```

## Q&A

https://hadoop.apache.org/docs/r1.0.4/cn/index.html

