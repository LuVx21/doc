<details>
<summary>点击展开目录</summary>
<!-- TOC -->

- [常见参数](#常见参数)
- [分区及副本分布](#分区及副本分布)
- [副本机制](#副本机制)
    - [Leader Epoch](#leader-epoch)
- [可靠性保证](#可靠性保证)

<!-- /TOC -->
</details>

## 常见参数

**message.max.bytes**

broker 所支持的最大消息大小

类似的还有`max.message.bytes`(主题级别)和`replica.fetch.max.bytes`

**replica.lag.time.max.ms**

Follower副本的LEO落后Leader LEO的最大时间, 超过了会认为 follower 不可用, 会踢出 ISR

## 分区及副本分布

副本分配逻辑规则如下:

* 在Kafka集群中, 每个Broker都有均等分配Partition的Leader机会.
* 每个Broker(按照BrokerId有序)依次分配主Partition,下一个Broker为副本, 如此循环迭代分配, 多副本都遵循此规则.

副本分配算法如下:

* 将所有N Broker和待分配的i个Partition排序.
* 将第i个Partition分配到第(i mod n)个Broker上.
* 将第i个Partition的第j个副本分配到第((i + j) mod n)个Broker上.

## 副本机制

分区预写日志

以分区为粒度的, 分区的预写日志被复制到 n 个服务器(1 个 leader + (n-1) 个 follower)

producer 只能往 leader 分区上写数据(读也只能从 leader 分区上进行), followers 只按顺序从 leader 上复制日志

每个分区的 leader 会维护一个 `in-sync replica`(同步副本列表, 又称 ISR)

消息成功复制到所有同步副本, 这条消息才算被提交, 因此慢副本的复制会影响吞吐量, 需要将这样的副本找出来从 ISR 中删除

以上可以看出生产者和消费者都只与leader副本进行交互



基于多分区多副本的设计机制, 可以让不同的分区位于不同的broker上, 提高了并发能力, 数据的安全性以及容灾可用性

这样所实现的数据可见性: 副本间同步完成的消息才能被看到

Follower副本消息同步完整流程:

1. Follower发送FETCH请求给Leader.
2. Leader会读取底层日志文件中的消息数据, 再更新它内存中的Follower副本的LEO值为FETCH请求中的`fetchOffset`值.
3. 尝试更新分区HW值. Follower接收到FETCH响应之后, 会把消息写入到底层日志, 接着更新LEO和HW值.

Leader和Follower的HW值更新时机是不同的, Follower的HW更新永远落后于Leader的HW. 这种时间上的错配是造成各种不一致的原因.
因此对于消费者而言, 消费到的消息永远是所有副本中最小的那个HW

**HW 机制存在的问题**

follow 的HW的作用之一就是在故障恢复时, 将 HW后的消息全部删除(更改 LEO 即可)然后从 leader 重新拉取, 如果没有追上 leader时 leader 出现故障, 可能这个 follower 就会升为 leader, 此时会导致数据丢失

加入 leader 和 follower 都重启, 且 follower 先重启成功, 此时升为 leader, 消费者写入消息, 此位置的消息就可能和原leader 的不一致

### Leader Epoch

弥补高水位机制的一些缺陷

Leader Epoch是一对值: (epoch,offset)

epoch: 代表当前 leader 的版本号, 从0开始, 当 Leader 变更过一次时, 我们的 epoch 就会 +1
offset: 该 epoch 版本的 Leader 写入第一条消息的位移

## 可靠性保证

可靠性的保证依靠以下几点:

**1. ack应答机制**

`acks`的配置支持三种级别, 可以针对数据可靠性和吞吐量进行权衡:

* `0`: 不等待broker的ack, broker收到消息未写入磁盘就发送ack, 此时Producer延迟最低, 存在极大的丢数据的可能性, 如leader故障, 此消息将会丢失
* `1`: 等待broker的ack, 在leader落盘成功后发送ack. 如果follower同步成功前, leader故障, 此消息将会丢失
* `-1`: 等待broker的ack, 在leader和所有follower全部落盘成功后发送ack, 具有最强的消息持久化保障. 如果同步完成后leader故障, 导致ack未成功发送, 会导致重发造成消息重复

阅读: [副本机制](../01.原理/04.副本机制.md)

**2. ISR**

上述`acks=-1`时, 如果某个follower故障, 迟迟不能和leader同步消息, 那么leader则会等待下去不会发送ack.

为解决此问题, 提出了`ISR(in-sync replica set)`方案:

每一个Partition都可能会有1个或者多个Replica, 其中一个被选举为Leader, 其他为Follower, leader则会跟踪与其保持同步的Replica列表, 该列表即为ISR(一个和leader保持同步的follower的列表). 当ISR中的所有follower完成同步就发送ack

如果某个follower长时间未和leader同步消息, 则将其从ISR中删除, 这个时间由`replica.lag.time.max.ms`参数配置

leader故障后, 则从ISR中的选举新的leader

**3. LEO/HW**

每个 kafka 副本对象都有两个重要的属性: LEO 和 HW, leader副本还保存了其他 Follower 副本的 LEO 值

LEO: `Log End Offset`, 每个副本最大的offset

HW: `High Watermark`, 消费者能见到的最大的 offset, HW之前的消息才对消费者可见, 为ISR中最小的LEO, 保证了 Partition 的 Follower 与 Leader 间数据的一致性

![hw](https://gitee.com/LuVx/img/raw/master/kafka/kafka_hw.png)
> ISR 中, `min(LEO)`到`max(LEO)`之间的消息还没有完全同步到所有副本中
>
> 位移值等于高水位的消息也属于未提交消息. 也就是说, 高水位上的消息是不能被消费者消费的

这是保证数据一致性的原理, 这里说的一致性, 是指分区各副本在发生 leader 变更前后, 消费者读到的数据是一致的




leader 和 follower 同步细节:

follower故障:

故障时会被踢出ISR, 待恢复后, 读取故障前的HW, 将大于HW的消息全部删除, 重新从leader开始同步

等到该follower的LEO大于等于分区此时的HW后, 即该follower已经追上了所有副本中的最慢的后, 再加入ISR中

leader故障:

故障时会从ISR中选出一个leader, 为了保证各副本间数据一致, 各个follower会将各自高于HW部分的消息删除掉, 然后从新leader同步消息

