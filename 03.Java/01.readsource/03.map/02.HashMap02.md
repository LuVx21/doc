<details>
<summary>源码-HashMap</summary>
<!-- TOC -->

- [结构](#结构)
- [put 过程](#put-过程)
    - [Hash方法](#hash方法)
    - [put 流程](#put-流程)
    - [链表与红黑树](#链表与红黑树)
- [get 过程](#get-过程)
- [线程安全](#线程安全)
    - [线程安全的Map](#线程安全的map)
- [fail-fast](#fail-fast)
- [序列化问题](#序列化问题)
- [Q&A](#qa)

<!-- /TOC -->

</details>

# 结构

```Java
// 默认容量
static final int DEFAULT_INITIAL_CAPACITY = 1 << 4;
// 默认扩容因子
static final float DEFAULT_LOAD_FACTOR = 0.75f;
// 链表重构为红黑树的控制因子
static final int TREEIFY_THRESHOLD = 8;
// 扩容临界值
int threshold;
// 节点类
static class Node<K,V> implements Map.Entry<K,V> {
    final int hash;// key的hash值
    final K key;
    V value;
    Node<K,V> next;// key的hash值相同时使用
}
transient Node<K,V>[] table;
transient Set<Map.Entry<K,V>> entrySet;
transient int size;
// 红黑树节点类,向上继承自上面的节点类
static final class TreeNode<K,V> extends LinkedHashMap.Entry<K,V> {
    TreeNode<K,V> parent;  // red-black tree links
    TreeNode<K,V> left;
    TreeNode<K,V> right;
    TreeNode<K,V> prev;    // needed to unlink next upon deletion
    boolean red;
    TreeNode(int hash, K key, V val, Node<K,V> next) {
        super(hash, key, val, next);
    }
}
```

* 默认初始化大小为16,默认使用75%的容量后进行扩容,变为原来的2倍(位运算左移1位)
* 遍历和插入顺序不同
* 存储容器为数组,即代码中的属性table,存储的是节点对象
* key可以为null,重复则会覆盖现有的(默认),也可以使用 `putIfAbsent()`方法不覆盖.value也可以为空
* 非线程安全
* 扩容后链表的顺序不再和Java7一样,不会反向
* 存在同一个槽位的key,其hash值可能并不相同,因为还和 `(capacity-1)`进行了与运算

底层:数组+链表(红黑树),初始化是可以自定义初始化大小和扩容因子(有参构造),使用静态内部类对象作为节点,重构后节点类型变为树节点类型,两者存在继承关系.

# put 过程

## Hash方法

```Java
static final int hash(Object key) {
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
// 计算下标
// (n - 1) & hash
```

上面是HashMap的hash算法,及获取下标的计算式,那么就有个问题: 为什么要自定义一个hash方法?

以默认大小16为例:

| 方法                  | 结果                                        | 十进制 |
| :-------------------- | :------------------------------------------ | -----: |
| `h=hashcode()`      | `0000 0000 0000 0001 1001 1110 0101 1111` | 106079 |
| `h >>> 16`          | `0000 0000 0000 0000 0000 0000 0000 0001` |      1 |
| `hash=h^(h >>> 16)` | `0000 0000 0000 0001 1001 1110 0101 1110` | 106078 |
| `n-1`               | `0000 0000 0000 0000 0000 0000 0000 1111` |     15 |
| `(n-1)&hash`        | `0000 0000 0000 0000 0000 0000 0000 1110` |     14 |

任何数和0异或都是其本身,因此h高16位不变,而数组的大小也是2的幂运算(2的k次方,k>=4),

因此下标由长度的低n位主导,这样会增大hash碰撞的几率,因此在与操作前先进行移位,异或操作使高16位能参与运算以降低碰撞几率.

数组长度很小时, hash 值的高位对于确定槽位没有太大的作用, 这样即使 hash 算法很好, 仅使用低位的几个用于确定槽位, 碰撞几率还是很高

再或者很好的 hash 算法, 但结果中高位差别很大, 但低位差别很小, 使用时也和差的 hash 算法差不多

## put 流程

以下重点分析大量hash碰撞发生时,链表向红黑树重构的过程:

添加用方法:

put putVal

```Java
final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) {
    Node<K,V>[] tab; Node<K,V> p; int n, i;
    if ((tab = table) == null || (n = tab.length) == 0)
        n = (tab = resize()).length;
    if ((p = tab[i = (n - 1) & hash]) == null)// 数组的对应位置没有元素就加进去
        tab[i] = newNode(hash, key, value, null);
    else {// 加进去的地方已经有值,可以简单的认为发生hash碰撞,其实hash值可能并不相同
        Node<K,V> e; K k;
        if (p.hash == hash &&
            ((k = p.key) == key || (key != null && key.equals(k))))// 键允许为空体现在这里,也考虑到使用相同的键添加元素情况
            e = p; // 同一个 key
        else if (p instanceof TreeNode)// 红黑树节点,往树上添加节点
            e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
        else {// 即不是空键也不是相同的键也没有重构成红黑树时
            for (int binCount = 0; ; ++binCount) {
                if ((e = p.next) == null) {
                    p.next = newNode(hash, key, value, null);// 继续往链表上加
                    if (binCount >= TREEIFY_THRESHOLD - 1) // >=7,即超过8个节点时(=7时,原有8个,本次新增一个,共有9个节点)重构
                        treeifyBin(tab, hash);// 重构操作,在内部将所有节点的类型转为树节点类型(原来的子类), 内部会检查数组长度是否>=64, 否则扩容而不重构为树
                    break;
                }
                if (e.hash == hash &&
                    ((k = e.key) == key || (key != null && key.equals(k))))
                    break;
                p = e;
            }
        }
        if (e != null) { // existing mapping for key
            V oldValue = e.value;
            if (!onlyIfAbsent || oldValue == null)
                e.value = value;// 旧值为 null会覆盖
            afterNodeAccess(e);// LinkedHashMap使用
            return oldValue;
        }
    }
    ++modCount;
    if (++size > threshold)// 扩容校验,超过容量的75%,容量的确定,倍增等都在此方法中实现
        resize();// 具体进行扩容的方法
    afterNodeInsertion(evict);// LinkedHashMap使用
    return null;
}
```

当数组需要扩容时,是调用 `resize()`方法,创建新数组并将旧数组的数据拷贝到新数组中

## 链表与红黑树

8之后引入了红黑树的概念, 表示若桶中链表元素超过8时, 会自动转化成红黑树; 若桶中元素小于等于6时, 树结构还原成链表形式

根本原因是为了解决 hash 冲突导致的读写效率降低问题

红黑树的平均查找长度是log(n), 假如树节点个数为8, 查找长度为log(8)=3, 链表的平均查找长度为n/2, 当长度为8时, 平均查找长度为8/2=4, 这才有转换成树的必要;

链表长度如果是小于等于6, 6/2=3, 虽然速度也很快的, 但是转化为树结构和生成树的时间并不会太短.

没有使用同一个值作为链表和树的边界, 也是增加缓冲, 避免频繁的增删导致结构来回转化

**链表->红黑树**

当链表超过8且数组长度超过64才会转为红黑树, 如果小于 64 仍会选择扩容, 这样能避免极端 hash 冲突下实际数组利用率不足情形

**红黑树->链表**

1. `removeTreeNode()`即删除树上元素时, 可能触发退化, 条件: `root == null || root.right == null || root.left == null || root.left.left == null`
2. `resize()`方法扩容后, 将数据从旧数组迁移到新数组时, `splite()`会分割成两棵树, 新树的节点数小于 6 时会退化为链表

> 退化方法: `TreeNode#untreeify`

# get 过程

查找用的方法:

```Java
final Node<K,V> getNode(int hash, Object key) {
    Node<K,V>[] tab; Node<K,V> first, e; int n; K k;
    if ((tab = table) != null && (n = tab.length) > 0 &&
        (first = tab[(n - 1) & hash]) != null) { // 计算出应该存储在数组的哪里,并获取到链上第一个节点
        if (first.hash == hash &&
            ((k = first.key) == key || (key != null && key.equals(k)))) // 判断第一个节点是否是get的对象
            return first;
        if ((e = first.next) != null) {// 不是第一个节点
            if (first instanceof TreeNode)// 此处TreeNode为树对象,满足则走树查找算法
                return ((TreeNode<K,V>)first).getTreeNode(hash, key);
            do { // 链表没有没有重构为红黑树,遍历整个链表
                if (e.hash == hash &&
                    ((k = e.key) == key || (key != null && key.equals(k))))
                    return e;
            } while ((e = e.next) != null);
        }
    }
    return null;
}
```

删除时也是类似,先判断数组中的是否是要查找的,不是则判断是链式结构还是树形结构进行查找,删除.

# 线程安全

**Java7死循环问题**

在Java7中,扩容完成后会执行下面的代码,其作用是将旧table的数据移到扩容后的新table中去:

[源码地址](https://github.com/lambdalab-mirror/jdk7u-jdk/blob/master/src/share/classes/java/util/HashMap.java#L589)

```Java
void transfer(Entry[] newTable, boolean rehash) {
    int newCapacity = newTable.length;
    for (Entry<K,V> e : table) {
        while(null != e) {// ②
            Entry<K,V> next = e.next;// ①
            if (rehash) {
                e.hash = null == e.key ? 0 : hash(e.key);
            }
            // 槽位index, 链式结构反转的前提
            int i = indexFor(e.hash, newCapacity);
            e.next = newTable[i];// ③
            newTable[i] = e;// ④
            e = next;
        }
    }
}
```

可以看出来,如果其中的链式节点经过rehash后仍然在同一槽位(即 `int i`的i相同),那么迁移后这2个节点顺序就会反向(由最后3行代码实现翻转).
假设有这样一个并发环境, 目前某槽位链中发生以下过程:

1. 线程1执行到①处, 同时线程2执行到②处, 此时线程1和2的变量 `e`为同一个对象, 此时满足 `e->next`的链式关系
2. 当线程1执行到④处结束后, 执行 `newTable[i] = e`, 此时 `e`节点成为新数组中对应槽位的首节点
3. 线程2开始执行③语句, 获取到新数组槽位的首节点, 设置了 `e.next=newTable[i]`, 就会出现 `e`指向的这个对象 `e.next=e`
4. 最终形成环状结构, 出现死循环, 即使这个死循环能够跳出来, 那么查找这个链表上的数据时也会出现死循环.

这种情况下可以使用使用jdk的命令确定错误的发生场所:`jps;jstack <pid>`,

值得注意的是,虽然是非线程安全的,但并不是在并发下使用就一定会出现这种情况.

---

Java8中不再使用上面的结构,扩容和数据迁移都是在 `resize()`方法中实现

[源码地址](https://github.com/openjdk/jdk8u/blob/master/jdk/src/share/classes/java/util/HashMap.java#L678):

```Java
Node<K,V> loHead = null, loTail = null;// 原下标位置的链表
Node<K,V> hiHead = null, hiTail = null;// 新位置的链表
Node<K,V> next;
do {
    next = e.next;
    // e 链表的第一个节点
    // 判断扩容前后槽位是否变化
    // 和jdk7不同, 不需要再计算hash值, 只需要使用原来的hash与上原来的长度(是个1带一串0, 目的就是判断hash值1对应的那位是0或1)
    if ((e.hash & oldCap) == 0) {
        if (loTail == null)
            loHead = e;
        else
            loTail.next = e;
        loTail = e;
    }
    else { // 槽位发生变化的key
        if (hiTail == null)
            hiHead = e;
        else
            hiTail.next = e;
        hiTail = e;
    }
} while ((e = next) != null);
if (loTail != null) {
    loTail.next = null;
    newTab[j] = loHead;
}
if (hiTail != null) {
    hiTail.next = null;
    newTab[j + oldCap] = hiHead;
}
```

上面这一段代码发生在扩容完成后,数据迁移时某个槽位内容为链表(内容为单元素,红黑树以外)时,可以看到是将一个链表分成了2个链表,新位置的确定也不再使用rehash,不会再出现环状结构.

另外,我们知道在同一个槽位的hash值不一定相同,那么什么样的key被放入哪个链表中呢?
下面以容量16扩容到32为例:

扩容前:

| 最大下标/key   | 二进制                                      |
| :------------- | :------------------------------------------ |
| `n-1(15)`    | `0000 0000 0000 0000 0000 0000 0000 1111` |
| `key1`       | `1111 1111 1111 1111 0000 1111 0000 0101` |
| `key1^(n-1)` | `0000 0000 0000 0000 0000 0000 0000 0101` |
| `key2`       | `1111 1111 1111 1111 0000 1111 0001 0101` |
| `key2^(n-1)` | `0000 0000 0000 0000 0000 0000 0000 0101` |

扩容后:

| 最大下标/key   | 二进制                                      |
| :------------- | :------------------------------------------ |
| `n-1(31)`    | `0000 0000 0000 0000 0000 0000 0001 1111` |
| `key1`       | `1111 1111 1111 1111 0000 1111 0000 0101` |
| `key1^(n-1)` | `0000 0000 0000 0000 0000 0000 0000 0101` |
| `key2`       | `1111 1111 1111 1111 0000 1111 0001 0101` |
| `key2^(n-1)` | `0000 0000 0000 0000 0000 0000 0001 0101` |

通过或运算计算槽位,但在扩容前key1和key2在同一个槽位,但扩容后就不在一个槽位了.

上面代码中有个条件 `(e.hash & oldCap) == 0`,满足这个条件的处在原槽位不动,不满足的则移动到 `原槽位+oldCap`的槽位.

但为什么满足这个条件的就是扩容前后槽位计算结果不会发生变化的呢?

因为容量是2的指数幂结果, 二进制表现就是只有 `指数+1`位为1,其他为0(一个1带一串0),

既然结果为0,那么hashcode的该位肯定为0, 因此扩容前后, 该位对于与运算的结果都没有影响, 对于定位槽位也没有影响, 即使扩容将 `容量-1`的该位从0变为了1.

对于那些槽位会发生变化的key来说, 其变化正好是增加了半个新容量(整个旧容量)的大小.

虽然Java8已经不会引起死循环但数据丢失的问题仍然存在,因此还是不推荐在并发环境下使用HashMap.

## 线程安全的Map

```Java
// 0. jdk提供
Map<String, String> map = new ConcurrentHashMap<>(8);
// 1. 工具类
Map<String, String> map = Collections.synchronizedMap(new HashMap<>(8));
// 2. Guava
Map<String, String> map = Maps.newConcurrentMap();
```

> HashTable也是安全的, 但使用 `synchronized`来保证的, put,get都会加锁, 高并发下性能低下

# fail-fast

HashMap的迭代器(Iterator)是 `fail-fast`迭代器, 而Hashtable的enumerator迭代器不是 `fail-fast`的.

所以当有其它线程改变了HashMap的结构(增减元素时), 将会抛出 `ConcurrentModificationException`,

但迭代器本身的remove()方法移除元素则不会抛出 `ConcurrentModificationException`异常.

值得注意的是这并不是一定发生的行为,它只是一种错误检测机制,不可能对是否出现并发修改作出任何保证.这条同样也是 `Enumeration`和 `Iterator`的区别

# 序列化问题

阅读源码知道,HashMaphe和ArrayList一样都实现了Serializable接口, 但最终的存储容器table确实被 `transient`修饰的,也就是说这个属性在序列化的时候被忽略,

原因我们知道是因为当一个key是自定义对象时,如果没有正确重写hashCode()方法的话,在不同环境下的hash值可能不同,这也就是推荐使用String等对象做key的原因.
另外就是table中有很多null的,这些null是没有必要序列化的,

序列化一个类的对象时,如果这个类实现了 `writeObject()`和 `readObject()`方法, 就会使用这两个方法进行序列化和反序列化,

没实现就使用 `defaultWriteObject()`和 `defaultReadObject()`方法

而HashMap实现了这两个方法:

```Java
private void writeObject(java.io.ObjectOutputStream s)
private void readObject(java.io.ObjectInputStream s)
```

# Q&A

**键**

要计算hashCode(),就要防止键改变,存取时键的值不同,其hashcode可能会不同,这样是不能正确取出值的.

因此使用不可变的, 声明作final的对象,并且采用合适的equals()和hashCode()方法的话,将会减少碰撞的发生,提高效率.

不可变性使得能够缓存不同键的hashcode,这将提高整个获取对象的速度,因此推荐使用String,Interger这样的wrapper类

因为包装类的一个对象一经创建,其所代表的值将不再变化,也就是说不能通过改变其引用来改变它的值,直至它被垃圾回收器回收

**Hashmap的容量为什么是2的幂次**

在计算下标时, 会使用到 `h & (len - 1) == h % len` 公式

当容量一定是 `2^n`时, 这个公式中的 `len - 1`在计算时候, 二进制表现全部为1, 在进行与操作的时候, 计算出的下标取决于Key的Hashcode值的最后几位

也就是说, key的后几位, 每一位都能够对计算的结果产生影响, 从而降低hash算法碰撞率, 假设其中的某个1变成了0, 那个key的对应位就无法起作用

其次, 位运算的效率是远远高于取余 `%`运算的(并不是绝对), 而2的n次幂是很好的支持位运算的

**初始大小为什么是16**

16应该是个经验值, 不能太大也不能太小, 大了使用不了浪费空间, 小了频繁扩容, rehash降低性能

实际上, 在创建时推荐使用公式 `expectedSize / 0.75 + 1` 计算初始值, 为什么是这个公式呢?

在 java8 之前, 数组的容量大小是大于传入大小的第一个 2 的 n 次幂

举个栗子, 假如要存储7个元素,于是传入7, 实际内部使用的是8, 而在存储6(`8*0.75`)个的时候就会发生扩容

而使用公式计算出设置为10(`7*4/3 + 1`), 实际使用的就是16, 不会在存储了6个后发生扩容

这个公式可以阅读 `putAll()`方法部分的源码

另外, Guava中提供了方法:

`Map<String, String> map = Maps.newHashMapWithExpectedSize(10);`

这个方法就使用了这个公式, 只需要传入预计存储的数量即可

**`Map.Entry TreeNode` 的关系**

HashMap.TreeNode -> LinkedHashMap.Entry -> HashMap.Node -> Map.Entry
