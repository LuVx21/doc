<details>
<summary>点击展开目录</summary>
<!-- TOC -->

- [幂等性](#幂等性)
- [id生成器](#id生成器)
- [如何实现15分钟内订单未付款则自动取消订单的功能？](#如何实现15分钟内订单未付款则自动取消订单的功能)
- [设计秒杀系统, 考虑正确性, 以及服务器不挂机如何设计, 怎么解决大并发](#设计秒杀系统-考虑正确性-以及服务器不挂机如何设计-怎么解决大并发)
- [设计一个简单的智能家具系统, 比如说加湿器和温湿度传感器关联, 怎么设计？考虑哪些点？](#设计一个简单的智能家具系统-比如说加湿器和温湿度传感器关联-怎么设计考虑哪些点)
- [以微博为例, 有1个亿的用户, 同时用户之间有关注和粉丝, 用户的关注和取关操作比较频繁, 如何设计架构和API接口](#以微博为例-有1个亿的用户-同时用户之间有关注和粉丝-用户的关注和取关操作比较频繁-如何设计架构和api接口)
- [在面对未知的流量暴增, 可以预先怎么处理](#在面对未知的流量暴增-可以预先怎么处理)

<!-- /TOC -->
</details>

# 幂等性

公式表示为: `F(F(x))=F(x)`

常见幂等:
* select查询天然幂等
* web的GET请求
* 特殊场景下的条件update, 条件delete删除

产生幂等问题的场景:
* 重复提交表单
* 重复发起请求

解决方案:
1. 前端控制可能产生幂等问题的操作, 避免重复操作
2. Post/Redirect/Get模式, 如提交表单后直接跳转至提交成功页面, 不提供重复提交的机会
3. session和表单中各自放入一个标识, 后端检查这两个标识, 满足设置的规则表示首次提交, 否则不处理
4. 操作数据库的场景下, 可借助悲观锁和乐观锁
5. redis分布式锁

# id生成器

- UUID
  - 优点: 独立生成, 性能好
  - 缺点: 生成的 ID 比较长, 而且无意义
- 雪花算法
  - 使用时间戳+机器码+序列号来生成
  - 优点: 有意义, 可排序
  - 缺点: 需要不同机器时间是同步的
- Redis incr
  - 优点: 不依赖数据库, 有序
  - 缺点: 增加复杂度(引入Redis组件)
- 数据库自增
  - 优点: 生成有序, 高可用, 实现简单
  - 缺点: 有性能瓶颈, 需要单独部署数据库实例

**中心服务生成**

中心服务生成可以保证 id 不重复, 缺点是性能低, 服务挂掉时拿不到 id.

**本地生成**

本地生成需要加锁, 或保证生成器生成的 id 是唯一的.

全局递增可以考虑用 redis increase 原子增来实现.

# 如何实现15分钟内订单未付款则自动取消订单的功能？

1. 单独存储, 支付后移动到订单表(可行性有限)
2. 查询取消(实时性要求不高的场景, 但工作量还是大)
3. 定时取消(实时性较高, 频繁创建定时任务)
4. 延时队列(最合适的, 取决于使用的产品,RocketMQ支持的时间较少,范围大)

# 设计秒杀系统, 考虑正确性, 以及服务器不挂机如何设计, 怎么解决大并发

- 流控
  - 请求流控: 参与秒杀的人远远大于实际成交的人数, 所以可以通过前端进行拦截, 限制最终流入系统的请求数量.
  - 客户端限流: 在客户端限制频繁操作, 比如说按钮5秒只能点击一次
  - 后端系统流控: 保证后端系统的压力维持在可正常处理的水平. 超过系统负载的请求, 可以直接拒绝.
  - 系统架构优化
    - 读取加速: 秒杀一般是读多写少的场景, 所以可以使用缓存分担数据库压力. CDN, 静态文件分离等.
    - 异步处理和排队: 通过异步处理任务, 消息队列来隔离前端的压力. 提高用户请求的响应速度.
    - 无状态服务设计: 实现无状态化的服务可以在秒杀活动前进行快速扩容. 比如说上云
- 单一职责原则: 使用微服务的设计思想, 秒杀服务单独部署.
- 秒杀链接加盐: URL 动态化, 通过 MD5 之类的加密代码随机字符串做 url, 然后前端通过先获取 url 再调用.
- Redis 集群: 集群, 主从同步, 读写分离, 哨兵
- Nginx 均衡负载, 恶意请求拦截
- 资源静态化: 使用 CDN 来分担压力
- 预加载库存到 Redis 中, 活动结束后再同步到数据库中. 使用 lua 脚本来解决读写分离的问题.
- 限流&降级&熔断&隔离
- 削峰填谷: 消息队列

# 设计一个简单的智能家具系统, 比如说加湿器和温湿度传感器关联, 怎么设计？考虑哪些点？

有中心服务器设备的话, 建立中心服务, 每一个智能设备入网需要在中心服务注册, 并按照协议上报自身的状态数据, 然后还需要对中心服务发送过来的命令进行接收处理.

无中心服务的话, 采用设备互相发现策略, 每一个设备入网后, 将广播自己的地址信息及使用的沟通协议. 其它设备在收到广播信息后加入到自己的的已发现列表中, 并设置定时查询, 当设备离线后, 用列表中移除. 然后设备之间使用特定的沟通协议, 进行交互.

# 以微博为例, 有1个亿的用户, 同时用户之间有关注和粉丝, 用户的关注和取关操作比较频繁, 如何设计架构和API接口

- 数据模型: `follow(uid1, uid2)`, `fans(uid2, uid1)`, `count(uid, 关注数, 粉丝数)`
- 架构设计
  - 数据库
    - 通过 uid `水平`分表, 建立数据冗余表(follow-fans两表)
    - redis存储互相关系(hash结构不是非常合适)
  - 计数服务: 异步计数关注数粉丝数(实时性要求不高, 数据准确性不严格高)
  - 模型
    - pull: 关注者访问新微博列表时, 查找他的关注者列表, 整理新微博显示出来
    - push: 新微博推送到每一个关注者的新微博列表中

# 在面对未知的流量暴增, 可以预先怎么处理
